
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Introduction}

% the code below specifies where the figures are stored
\ifpdf
    \graphicspath{{1_introduction/figures/PNG/}{1_introduction/figures/PDF/}{1_introduction/figures/}}
\else
    \graphicspath{{1_introduction/figures/EPS/}{1_introduction/figures/}}
\fi

% ----------------------------------------------------------------------
%: ----------------------- introduction content ----------------------- 
% ----------------------------------------------------------------------



%: ----------------------- HELP: latex document organisation
% the commands below help you to subdivide and organise your thesis
%    \chapter{}       = level 1, top level
%    \section{}       = level 2
%    \subsection{}    = level 3
%    \subsubsection{} = level 4
% note that everything after the percentage sign is hidden from output

Finding bugs in binaries is hard \cite{aflfuzzer, bohme2017coverage}. Therefore a technique called `fuzzing' is a major contributor to finding bugs in software. Fuzzing or fuzz testing is the process of generating inputs and running a target binary with this input to see if it crashes. However, this can be done in different ways, with very different strategies. In this thesis we look at different strategies to mutate an input to see if it covers new states in the program and compare these strategies. 

Many different fuzzers are created recently \cite{bohme2017coverage, bohme2017directed, osterlund2020parmesan, kersten2017poster}. Most of these contain improvements, different strategies or try to check common coding errors and they can be completely different from other fuzzers in how they approach their input generation, how they classify an input as desired and how they monitor the state of the target program. The thesis is inspired by a blog post \cite{angora2020blogpost} about the state-of-the-art Angora fuzzer \cite{chen2018angora}, where a suggestion is given that their new gradient descent mutation strategy does not contribute as much to the performance of the fuzzer as the paper suggests. One explanation for this could be that the fuzzer uses so many other optimisations, that these cause the improved performance.
In order to measure the performance of a strategy, we measure which metric increases the coverage of a program, since this is a common benchmark \cite{aflfuzzer, metzman2020fuzzbench, aizatsky2016ossfuzz}. Coverage is increased when new program paths are found, hence, if a condition in a program can be flipped. So we measure on a condition level if it is flipped.
We decided against testing a directed fuzzer, because this leads to less generalizable results, since the results would then only be valid for the chosen weight of the conditions.
To get insight in the conditions of a binary, we need some runtime instrumentation, so we focus our thesis on greybox fuzzers. We only consider mutational strategies which mutate an input which intent to flip a specific condition.
For our thesis we mutate inputs using different strategies and see how they compare.

We answer the research question: What are the best mutation strategies of a coverage-guided fuzzer? 

During this process we study the correlations between dynamic and static metrics of the conditions and the performance of the strategy on a condition level, with the ultimate goal of creating a strategy scheduler to select the most effective strategy based on the metrics of a condition.

To test multiple conditions, we need a dataset of traces which include conditions from several different binaries.
We first collected several traces, using the Angora fuzzer. A trace generated by Angora contains all conditions seen in the execution, including dynamic taint tracking information. A possible issue with this dataset might be that the found conditions might be biased by the mutators used by the Angora fuzzers. We then create a framework to analyse these \texttt{(input, trace)} pairs, where we apply several mutation strategies on the \texttt{input} for every \texttt{condition} in a \texttt{trace} which was not seen before. We record which strategy flips which conditions. From these results, we study which strategies flip which conditions better then others. Results from \cite{lyu2019mopt} suggest that different mutation operators for a random mutation differ in effectiveness based on the binary on which the fuzzer is ran. We try to get more insight in the reason for this, by looking at exactly which mutation strategy and substrategy flipped a branch using microbenchmarks, instead of macrobenchmarks as in \cite{klees2018evaluating}.

We find that the conclusions from \cite{lyu2019mopt} also extend to the more complex mutation strategies like gradient descent and concolic execution. More complex mutation strategies differ in effectiveness between different binaries. 
Getting different results from different binaries suggests that the program structure might have more influence on the effectiveness of a strategy then can be found by the used static and dynamic metrics, which are more targeted towards a given condition. However, the gradient descent algorithm can increase the coverage when dynamic taint information is available, since we found over 90\% overlap between the flipped conditions by this strategy and all other strategies except one. When this dynamic taint information is not available, a random mutator outperforms more sophisticated strategies like a concolic execution run on 6 of the 7 tested binaries. The addition of a mutator which changes the length of an input can help find new coverage, since the flipped conditions found by this strategy are not easily found by most other strategies.

With this data we constructed a program-specific machine learning model, which can help the scheduling algorithm of a fuzzer by predicting which conditions are more likely to be flipped. We created models for every binary with an accuracy of around 80\% for 6 of the 7 tested binaries.
We failed however to create a model which can predict the strategy which flips a condition the fastest or predict the time it will take for a strategy to flip a condition, since dummy models outperformed the created models.

\paragraph{Contributions}
In this thesis, we create a framework to micro-benchmark the performance of mutation strategies of coverage-guided fuzzers. We use this framework to test several mutation strategies on several binaries and analyse the results using different metrics to measure which strategies perform better than others.
We find that depending on the taint information available, the gradient descent mutator is a good choice which can outperform more sophisticated strategies. When no information is available, the random mutator is a good alternative.
We also constructed a program-specific machine learning model to predict which conditions will be flipped. This approach can be used to improve schedulers in existing fuzzers.



\begin{comment}
We will adjust angora to output the trace and the input and expected output at a branch.
We will then rerun the trace while we fuzz the input and see if we can flip the branch using different fuzzing stategies.
We will do this by converting the program to LLVM, then with clang we instrument the program using the AST representation at the comparison instructions to see if we still follow the same trace based on the generated input.

Possible stategies:
concolic execution (KLEE?)
magic byte extraction
input-state correspondence
random guessing
mutational fuzzing
evolutional fuzzing
gradient descend

possible when time:
symbolic execution (KLEE)
use neural nets (NEUEX and NEUZZ)

These are microbenchmarks. We do not look at how strategies help finding interesting branches.
Completely context aware branch based strategy from angora

Fuzzers are a bag of tricks, try to evaluate part of the tricks, maybe even create fuzzer which adopts strategy based on `function shape'

How to determine function shape?


Step 1:
Modify Angora to get (trace, input, output)
Step 2:
Write instrumentation tool to run program with input and check trace
Step 3:
Write strategies
Step 4:
Compare strategies
Step 5:
Determine function shape
Step 6:
Rate strategies based on function shape
Step 7:
Create fuzzer based on function shape/strategy
 
\end{comment}